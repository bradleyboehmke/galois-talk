---
title: "Galois Tech Talk"
author: "Brad Boehmke"
date: "2021-02-05"
output:
  xaringan::moon_reader:
    css: ["custom.css", "style.css"]
    self_contained: false
    lib_dir: libs
    chakra: libs/remark-latest.min.js
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    seal: false
---

class: alt-title-slide   

<br><br><br><br><br><br><br>
.font200[Feature Importance]

.font140[A road to clarity]

<br>
.font110[Bradley Boehmke<br>Feb 5, 2021<br>[bit.ly/bb_galois](http://bit.ly/bb_galois)]


---
class: section-slide 

<br><br><br><br><br><br><br><br><br><br><br>
# .font150[Setting expectations]

---
class: no-header-slide

<br><br>

.pull-left.font110[

]

<br>
.font140.pull-right[

* Who am I

* General research interests

* Interpretable machine learning

]

---
class: no-header-slide

<br><br>

.pull-left.font110[

### TL;DR

* Data scientist (1) | Software engineer (2)

* Interested in applied ML research

* Build open source tools for other data scientists

]

<br>
.opacity10.font140.pull-right[

* Who am I

* General research interests

* Interpretable machine learning

]

---
class: section-slide 

<br><br><br><br><br><br><br><br><br><br><br>
# .font150[Who am I]

---

# About me  <img src="figures/about_me_bitmoji.png" height="60" width="60", align="right">

<br>

.pull-left[

```{r, echo=FALSE}
knitr::include_graphics("figures/name-tag.png")
```

]

.pull-right[

```{r, echo=FALSE, out.width="80%", out.height="80%"}
knitr::include_graphics("figures/fam.png")
```

]

---

# About me  <img src="figures/about_me_bitmoji.png" height="60" width="60", align="right">

<br>

.pull-left[

```{r, echo=FALSE}
knitr::include_graphics("figures/name-tag.png")
```

]

.pull-right[

```{r map-home, echo=FALSE}
#library(ggmap)
library(leaflet)
#register_google("AIzaSyBnYq_MgT5dwW9FJrkESKBVtUWcM_UF4Zg")

home <- tibble::tibble(
   Longitude = c(-84.015850),
   Latitude = c(39.692720)
   )

#us <- get_map(location='united states', zoom=4, maptype = "satellite",
#              source='google',color='color')
# 
#ggmap(us) + 
#  geom_point(data = home, aes(x = Longitude, y = Latitude)) +
#  theme(axis.title = element_blank(),
#        axis.text = element_blank(),
#        axis.ticks = element_blank())
# 
# home %>% 
#   qmplot(Longitude, Latitude, data = ., 
#          maptype = "toner-background", darken = .7, size = I(2.5)) + 
#   theme(axis.title = element_blank(),
#         axis.text = element_blank())

leaflet() %>% 
  addTiles() %>%
  setView(lat = 39.692720, lng = -84.015850, zoom=5) %>%
  addMarkers(lng=-84.015850, lat=39.692720)

#leaflet() %>% 
#  addProviderTiles('Esri.WorldImagery') %>%
#  setView(lat = 39.692720, lng = -84.015850, zoom=5) %>%
#  addMarkers(lng=-84.015850, lat=39.692720)
```

]

---

# About me  <img src="figures/about_me_bitmoji.png" height="60" width="60", align="right">

<br>

.pull-left[

```{r, echo=FALSE}
knitr::include_graphics("figures/name-tag.png")
```

]

.pull-right[

<img src="figures/ndsu.png" align="middle" alt="NDSU" width="150" /> &ensp;&ensp; .font120[B.S. Kinesiology]

<br>

<img src="figures/afit.png" align="middle" alt="AFIT" width="150" /> &ensp;&ensp; .font120[M.S. Cost Analytics]

<br>

<img src="figures/afit.png" align="middle" alt="AFIT" width="150" /> &ensp;&ensp; .font120[PhD Logistics]

]

---

# About me  <img src="figures/about_me_bitmoji.png" height="60" width="60", align="right">

<br>

.pull-left[

```{r, echo=FALSE}
knitr::include_graphics("figures/name-tag.png")
```

]

.pull-right[

Air Force &ensp; &ensp; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#62; Finance Officer

Booz Allen Hamilton &ensp; &#8729; &#8729; &#8729; &#62; Cost Analyst

Air Force A9 &ensp; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#62; OR Analyst

AFIT &ensp; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#62; Professor

University of Cincinnati &ensp; &#8729; &#62; Adjunct

Ascend Innovations  &ensp; &#8729; &#8729; &#8729; &#8729; &#62; Data Scientist

84.51Â° &ensp; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#8729; &#62; Director, Data Science

]

---

# About me  <img src="figures/about_me_bitmoji.png" height="60" width="60", align="right">

<br>

.pull-left[

```{r, echo=FALSE}
knitr::include_graphics("figures/name-tag.png")
```

]


.pull-right.scrollable90[

* Intro to `r fontawesome::fa("r-project")` [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/uc-r/Intro-R)
* Intro to `r fontawesome::fa("python")` [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/uc-python/intro-python-datasci)
* Intermediate `r fontawesome::fa("r-project")` [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/uc-r/Intermediate-R)
* Intermediate `r fontawesome::fa("python")` [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/uc-python/intermediate-python-datasci)
* Data Wrangling with `r fontawesome::fa("r-project")` [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](http://uc-r.github.io/data_wrangling)
* Machine Learning with `r fontawesome::fa("r-project")` [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/uc-r/Advanced-R)
* Advanced `r fontawesome::fa("python")` [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/uc-python/advanced-python-datasci)
* Deep Learning with Keras and TensorFlow [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/rstudio-conf-2020/dl-keras-tf)
* Delivering Data Science Products via Packages [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/misk-data-science/misk-packages)
* Text Mining with `r fontawesome::fa("r-project")` [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/bradleyboehmke/R-Training-Text-Mining)
* Interpretable Machine Learning [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/bradleyboehmke/CinDay-RUG-IML-2018)
* DevOps for Data Science (internal)


]

---

# About me  <img src="figures/about_me_bitmoji.png" height="60" width="60", align="right">

<br>

.pull-left[

```{r, echo=FALSE}
knitr::include_graphics("figures/name-tag.png")
```

]

.pull-right[

```{r, echo=FALSE, out.width="49%", out.height="20%", fig.show='hold'}
knitr::include_graphics(c("https://images-na.ssl-images-amazon.com/images/I/41ttvv4UJ%2BL._SX331_BO1,204,203,200_.jpghttps://images.tandf.co.uk/common/jackets/amazon/978113849/9781138495685.jpg", "https://images.tandf.co.uk/common/jackets/amazon/978113849/9781138495685.jpg"))
```

```{r, eval=FALSE, echo=FALSE, out.height=400}
knitr::include_graphics("https://images.tandf.co.uk/common/jackets/amazon/978113849/9781138495685.jpg")
```

]


---
# About me  <img src="figures/about_me_bitmoji.png" height="60" width="60", align="right">

<br>

.pull-left[

```{r, echo=FALSE}
knitr::include_graphics("figures/name-tag.png")
```

]

.pull-right[

<br><br>

```{r, echo=FALSE}
knitr::include_graphics("figures/r-contributions.png")
```

]

---

# About me  <img src="figures/about_me_bitmoji.png" height="60" width="60", align="right">

<br>

.pull-left[

```{r echo=FALSE}
knitr::include_graphics("figures/name-tag.png")
```

]


.font120.pull-right[
<br><br>
[`r fontawesome::fa("globe", fill = "steelblue")`](http://bradleyboehmke.github.io/) bradleyboehmke.github.io  <br>
[`r fontawesome::fa("github", fill = "steelblue")`](https://github.com/bradleyboehmke/) @bradleyboehmke  <br>
[`r fontawesome::fa("twitter", fill = "steelblue")`](https://twitter.com/bradleyboehmke) @bradleyboehmke  <br>
[`r anicon::aia("google-scholar", color = "steelblue", animate = FALSE)`](http://bit.ly/bradleyboehmke_scholar) bit.ly/bradleyboehmke_scholar <br>
[`r fontawesome::fa("envelope", fill = "steelblue")`](mailto:bradleyboehmke@gmail.com) bradleyboehmke@gmail.com  

]


---
class: section-slide 

<br><br><br><br><br><br><br><br><br><br><br>
# .font150[Research]

---
# Focus areas

<br>
1. ML application for resource allocation

2. ML interpretability
   
3. Open source tool building

4. Inner source ecosystem (day-to-day)

---
# Focus areas

<br>
1. .bold[ML application for resource allocation]

   - Cyber anomaly detection [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://journals.sagepub.com/doi/full/10.1177/1748301818791503)
   - Strategic purchasing [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://www.sciencedirect.com/science/article/abs/pii/S1478409217300687)
   - Humanitarian logistics [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://link.springer.com/article/10.1007/s10479-017-2607-z)
   - Force structure impacts to cost [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://www.tandfonline.com/doi/abs/10.1080/1941658X.2016.1155186)
   - Performance trend analysis [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://www.tandfonline.com/doi/abs/10.1080/1941658X.2015.1064046)
   - Performance efficiency analysis [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://www.jstor.org/stable/26296084?seq=1)

---
# Focus areas

<br>
1. ML application for resource allocation

2. .bold[ML interpretability]

   - Feature importance measures [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://arxiv.org/abs/1805.04755)
   - Residuals & diagnostics for binary & ordinal regression [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C6&q=Residuals+and+diagnostics+for+binary+and+ordinal+regression+models%3A+An+introduction+to+the+sure+package.&btnG=)

---
# Focus areas

<br>
1. ML application for resource allocation

2. ML interpretability
   
3. .bold[Open source tool building]

  - vip package [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/koalaverse/vip)
  - sure package [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/koalaverse/sure)
  - anomalyDetection package [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/koalaverse/anomalyDetection)
  - KraljicMatrix package [`r fontawesome::fa("link", height = 15, fill = 'lightgrey')`](https://github.com/koalaverse/KraljicMatrix)<br>
  
---
# Focus areas

<br>
1. ML application for resource allocation

2. ML interpretability
   
3. Open source tool building

4. .bold[Inner source ecosystem]

  - Poirot: automated Spark environment configuration
  - Effodata: data ingestion & wrangling
  - KPI: common metric computation
  - UPC Input: unique UPC analysis
  - Package Template: internal package generator
  - ...
  
---
# Today's focus

<br>
1. ML application for resource allocation

2. ML interpretability
 
  - .bold.blue[Feature importance measures]: advance model-agnostic variable importance measures
    
3. Open source tool building

  - .bold.blue[vip package]: provide a consistent interface for variable importance analysis

4. Inner source ecosystem


---
class: section-slide 

<br><br><br><br><br><br><br><br><br><br><br>
# .font150[Interpretable machine learning]

---
class: section-slide 

<br><br><br><br><br><br><br><br><br><br><br>
# .font150[Interpretable machine learning]

.white.font120.center[What does this mean to you?]

---

# A mental model


.pull-left[

.bolder.font120[Philosophical: Political & Social]

* Data ethics
* Fairness, Accountability, Transparency (FAT)
* Regulatory examples:
   - Civil Rights Acts
   - Americans with Disabilities Act
   - Genetic Information Nondiscrimination Act
   - Health Insurance Portability and Accountability Act
   - Equal Credit Opportunity Act
   - Fair Credit Reporting Act
   - Fair Housing Act
   - European Union Greater Data Privacy Regulation
* https://www.fatml.org/resources/relevant-scholarship

.center.font120.blue[___Right to explanation___]
]


.pull-right[

```{r santa, echo=FALSE}
knitr::include_graphics("figures/santa.jpg")
```

]


---

# A mental model


.pull-left[

```{r blackbox, echo=FALSE}
knitr::include_graphics("figures/black-box.gif")
```

]


.pull-right[

.bolder.font120[Pragmatic: Model Logic]

* Performance analysis
  - Residual plots
  - Lift charts
  - ROC curves
* Sensitivity analysis
  - Simulated data
  - Perturbation
  - Accuracy vs explanation
* Feature (aka variable, predictor) analysis
  - Feature importance
  - Feature effects

<hr style="height:40px; visibility:hidden;" /></hr>

.center.font120.blue[___Ability to explain___]
]

---

# Today's focus


.pull-left[

.opacity10[
.bolder[.font120[Philosophical: Political & Social]]

* Data ethics
* Fairness, Accountability, Transparency (FAT)
* Regulatory examples:
   - Civil Rights Acts
   - Americans with Disabilities Act
   - Genetic Information Nondiscrimination Act
   - Health Insurance Portability and Accountability Act
   - Equal Credit Opportunity Act
   - Fair Credit Reporting Act
   - Fair Housing Act
   - European Union Greater Data Privacy Regulation
* https://www.fatml.org/resources/relevant-scholarship

.center.font120[___Right to explanation___]
]]

.pull-right[

.bolder.font120[Pragmatic: Model Logic]

.opacity10[
* Performance analysis
  - Residual plots
  - Lift charts
  - ROC curves
* Sensitivity analysis
  - Simulated data
  - Perturbation
  - Accuracy vs explanation]
* Feature analysis
  - Feature importance

<hr style="height:20px; visibility:hidden;" /></hr>

.center.font120.blue[___Ability to explain___]
]

]

---
class: section-slide 

<br><br><br><br><br><br><br><br><br><br><br>
# .font150[Feature Importance]

---
# Why we care

* Having a model that predicts well is only solving part of the problem

* For most problems, it is just as important, if not more, to extract information about the relationships uncovered by the learning algorithm

--

  - _Why_ do we consider this activity to be anomalous?
  
  - _Why_ do we expect a customer to churn?
  
  - _Why_ do we expect the capital investment to produce great returns?
  
  - _Why_ do we suspect that person in the image is a threat?

--

<br>

.center.content-box-grey[Decision makers want to know why and our features can be a _starting_ point.]

---
# Current problems

* Model-based methods

   - Examples:
      - Linear regression: t-statistic or standardized coefficients
      - Multi-adaptive regression splines: reduction in GCV
      - Tree-based algorithms: average reduction in SSE
      - Neural nets: a product of neuron weights across all layers
--
      - <font style="color:red">Support vector machines: ???</font>
      - <font style="color:red">Naive Bayes: ???</font>
--
   - Not comparable (i.e. <font style="color:red">Stacked models</font>)
   - Not intuitive

---
# Current problems

* Model-based methods

* Existing model-agnostic approaches

   - Permutation
      - Unclear whether you should use training or test data
      - Requires truth, not just prediction
      - Linked to the error of the model
      - Can be skewed by correlated features
      - Random shuffling = Lack of reproducibility
      - Can have scaling issues
    
    - Shapley values
      - Unintuitive explanations
      - Traditional implementation does not scale
      - TreeSHAP is fast...but model dependent

---
# Current problems

* Model-based methods

* Existing model-agnostic approaches

* Inconsistent implementations

--

<br><br><br><br>

.center.bold[_Data scientists need better approaches to analyze feature importance._]

---
class: section-slide 

<br><br><br><br><br><br><br><br><br><br><br>
# .font150[A solution]


---
# A solution

<br>

.pull-left[

.bold[Necessary attributes]

- Simple model-agnostic approach

- Easy to interpret

- Scales well

- Detects interactions

- Creates a consistent implementation

]

--

.pull-right[

.bold[Result]

1. PDP-based feature importance measure

2. Open source tool for feature importance analysis

]

---
# Prerequisite knowledge

What is a .bold[P]artial .bold[D]ependence .bold[P]lot (aka PDP)?

> _Provides the marginal effect of a feature (or subset thereof) on the predicted outcome._

--

First, let's talk about a different concept that will make this easier to understand.

---
# Prerequisite knowledge

What are .bold[I]ndividual .bold[C]onditional .bold[E]xpectation curves (aka ICE curves)? 

--

<br>

.pull-left[

Say we have the given data:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(kableExtra)

# Simulate training data
set.seed(101)  # for reproducibility
trn <- as.data.frame(mlbench::mlbench.friedman1(500))  # ?mlbench.friedman1

# Inspect data
tibble::as_tibble(trn) %>%
  mutate_all(round, 2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F, font_size = 12)
```

]

.pull-right[

And any given predictive model:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(randomForest)

set.seed(101)  # for reproducibility
any_model <- ppr(y ~ ., data = trn, nterms = 10)
```

```{r, collapse=TRUE, error=TRUE}
# in this example it happens to be a projection 
# pursuit regression
any_model #<<

# which has no model-specific variable importance measures
vip::vip(any_model)
```


]

---
# Prerequisite knowledge

What are .bold[I]ndividual .bold[C]onditional .bold[E]xpectation curves (aka ICE curves)? 

<br>

.pull-left[

When we make predictions, the output is based on our feature values

```{r, echo=FALSE, message=FALSE, warning=FALSE}
trn %>% 
  mutate(y_hat = predict(any_model)) %>%
  mutate_all(round, 2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F, font_size = 12) %>%
  column_spec(12, background = "yellow")
```

]

.pull-right[

```{r, echo=FALSE, fig.align='center'}
library(DiagrammeR)

mermaid("
graph LR
B[Features]
B-->C{Model}
C-->D[Predictions]
")
```

]

---
# Prerequisite knowledge

What are .bold[I]ndividual .bold[C]onditional .bold[E]xpectation curves (aka ICE curves)? 

<br>

.pull-left[

Therefore, to understand how `x.1` influences the predicted values we can assess predicted values while holding `x.1` constant at certain values.


```{r, collapse=TRUE}
# quantiles
summary(trn[["x.1"]])
```

]

--

.pull-right[

E.g. - when `x.1 = 0.002379` (the min)

```{r, echo=FALSE, message=FALSE, warning=FALSE}
min_val <- trn %>% 
  mutate(x.1 = min(x.1))

min_val %>%
  mutate(y_hat = predict(any_model, newdata = min_val)) %>%
  mutate_at(vars(-one_of("x.1")), round, 2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F, font_size = 12) %>%
  column_spec(c(1, 12), background = "yellow")
```

]

---
# Prerequisite knowledge

What are .bold[I]ndividual .bold[C]onditional .bold[E]xpectation curves (aka ICE curves)? 

<br>

.pull-left[

Therefore, to understand how `x.1` influences the predicted values we can assess predicted values while holding `x.1` constant at certain values.


```{r, collapse=TRUE}
# quantiles
summary(trn[["x.1"]])
```

]

.pull-right[

E.g. - when `x.1 = 0.500790` (the median)

```{r, echo=FALSE, message=FALSE, warning=FALSE}
med_val <- trn %>% 
  mutate(x.1 = median(x.1))

med_val %>%
  mutate(y_hat = predict(any_model, newdata = med_val)) %>%
  mutate_at(vars(-one_of("x.1")), round, 2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F, font_size = 12) %>%
  column_spec(c(1, 12), background = "yellow")
```

]

---
# Prerequisite knowledge

What are .bold[I]ndividual .bold[C]onditional .bold[E]xpectation curves (aka ICE curves)? 

<br>

.pull-left[

Therefore, to understand how `x.1` influences the predicted values we can assess predicted values while holding `x.1` constant at certain values.


```{r, collapse=TRUE}
# quantiles
summary(trn[["x.1"]])
```

]

.pull-right[

E.g. - when `x.1 = 0.999288` (the max)

```{r, echo=FALSE, message=FALSE, warning=FALSE}
max_val <- trn %>% 
  mutate(x.1 = max(x.1))

max_val %>%
  mutate(y_hat = predict(any_model, newdata = max_val)) %>%
  mutate_at(vars(-one_of("x.1")), round, 2) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F, font_size = 12) %>%
  column_spec(c(1, 12), background = "yellow")
```

]

---
# Prerequisite knowledge

What are .bold[I]ndividual .bold[C]onditional .bold[E]xpectation curves (aka ICE curves)? 

<br>

.pull-left[

If we do this across a bunch of values for `x.1` we get an ICE curve plot that shows .bold[_the effect of a feature on the predicted outcome_] assuming all other features are constant.

> E.g. _As each customer's income increases, how does this effect expected total spend?_

]

.pull-right[

```{r, echo=FALSE, warning=FALSE, fig.height=5, fig.width=5}
library(pdp)

ice <- partial(any_model, pred.var = "x.1", ice = TRUE)

p1 <- autoplot(ice, alpha = 0.1) +
  ylim(range(trn$y)) +
  theme_light()

p1
```


]

---
# Prerequisite knowledge

What are .bold[I]ndividual .bold[C]onditional .bold[E]xpectation curves (aka ICE curves)? 

<br>

.pull-left[

And that red line, that's the average effect of `x.1` on predicted values across all observations.

That is referred to as the .bold[_"partial dependence"_].

]

.pull-right[

```{r, echo=FALSE, warning=FALSE, fig.height=5, fig.width=5}
p1
```


]

---
# Prerequisite knowledge

```{r, echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=9, fig.align='center'}
features <- paste0("x.", 1:10)

ice_curves <- lapply(features, FUN = function(feature) {
  ice <- partial(any_model, pred.var = feature, ice = TRUE)
  autoplot(ice, alpha = 0.1) + 
    ylim(range(trn$y)) +
    theme_light()
})
grid.arrange(grobs = ice_curves, ncol = 5, top = "ICE Curves")
```


```{r, echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=9, fig.align='center'}
pdps <- lapply(features, FUN = function(feature) {
  pd <- partial(any_model, pred.var = feature)
  autoplot(pd) + 
    ylim(range(trn$y)) + 
    theme_light()
})
grid.arrange(grobs = pdps, ncol = 5, top = "PDP")
```

---
# Prerequisite knowledge

<br>

.center.bold[_When you look at this plot, which features appear influential (aka important) and which do not?_]


```{r, echo=FALSE, warning=FALSE, fig.height=3.5, fig.width=9, fig.align='center'}
grid.arrange(grobs = pdps, ncol = 5, top = "PDP")
```

--

* Plots with more variance mean those features are more influential on the predicted value.

* Plots that are flat have very little influence on the predicted value.

---
# PDP-based VIP

.font110[
Partial dependence is estimated as:

$$\bar{f}_s(z_s) = \frac{1}{n}\sum^n_{i=1} \hat{f}(z_s, z_{i,c})$$
]

--

.font110[
Then we measure importance based on flatness ( $F(\cdot)$ ):

$$i(x) = F(\bar{f_s}(z_s))$$
]

--

.font110[
And a simple measure of $F(\cdot)$  is standard deviation for continuous predictors and the range statistic for categorical variables:


$$i(x_1) = \begin{cases}
    \sqrt{\frac{1}{k-1} \sum^k_{i=1} [\bar{f}_1(x_{1i}) - \frac{1}{k} \sum^k_{i=1} \bar f_1(x_{1i})]^2 } & \text{if } x_1 \text{ is continuous}\\
    [\text{max}_i(\bar f(x_{1i})) - \text{min}_i (\bar{f}_1(x_{1i}))] / 4 & \text{if } x_1 \text{ is categorical}
\end{cases}$$

]

---
# PDP-based VIP

.pull-left[

```{r, echo=FALSE, warning=FALSE, fig.height=7.5, fig.width=5, fig.align='center'}
pdps <- lapply(features, FUN = function(feature) {
  pd <- partial(any_model, pred.var = feature)
  autoplot(pd) + 
    ylim(range(trn$y)) + 
    theme_light()
})

pdp_rearranged <- list(pdps[[4]], pdps[[2]], pdps[[1]], pdps[[5]], pdps[[3]],
                       pdps[[6]], pdps[[9]], pdps[[10]], pdps[[7]], pdps[[8]])

grid.arrange(grobs = pdp_rearranged, ncol = 2, top = "PDP")
```

]

.pull-right[

<br>

```{r, echo=FALSE}
vip::vi(any_model, method = "firm") %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = F, font_size = 17)
```


]

---
# Detecting interactions

.pull-left.font80[

- Unfortunately, PDPs can often hide interaction effects.

- Our data: $$Y = 10 \sin\left(\pi x_1 x_2\right) + 20 \left(x_3 - 0.5\right) ^ 2 + 10 x_4 + 5 x_5 + \epsilon$$

]

---
# Detecting interactions

.pull-left.font80[

- Unfortunately, PDPs can often hide interaction effects.

- Our data: $$Y = 10 \sin\left(\pi x_1 x_2\right) + 20 \left(x_3 - 0.5\right) ^ 2 + 10 x_4 + 5 x_5 + \epsilon$$

- Our PDP-based VIP approach does a better job identifying actual influential features than established methods

- But it doesn't highlight the interaction effect

]

.pull-right[

<br><br>

```{r, echo=FALSE, fig.height=5}
library(NeuralNetTools)  # for various tools for neural networks
library(nnet)            # for fitting neural networks w/ a single hidden layer

# Simulate data
set.seed(101)  # for reproducibility
trn <- as.data.frame(mlbench::mlbench.friedman1(n = 500, sd = 1))

# Fit a neural network to simulated data
set.seed(103)
nn <- nnet(y ~ ., data = trn, size = 8, linout = TRUE, decay = 0.01,
           maxit = 1000, trace = FALSE)

# VIP: partial dependence algorithm
p1 <- vip::vip(nn, method = "firm", feature_names = paste0("x.", 1:10)) +
  labs(x = "", y = "Importance", title = "PDP method") +
  theme_light()

# VIP: Garson's algorithm
nn_garson <- garson(nn, bar_plot = FALSE) %>%
  tibble::rownames_to_column("Variable") %>%
  select(Variable, Importance = rel_imp) %>%
  mutate(Importance = Importance)
p2 <- ggplot(nn_garson, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col() +
  labs(x = "", y = "Importance", title = "Garson's method") +
  coord_flip() +
  theme_light()

# VIP: Olden's algorithm
nn_olden <- olden(nn, bar_plot = FALSE) %>%
  tibble::rownames_to_column("Variable") %>%
  select(Variable, Importance = importance) %>%
  mutate(Importance = Importance)
p3 <- ggplot(nn_olden, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col() +
  labs(x = "", y = "Importance", title = "Olden's method") +
  coord_flip() +
  theme_light()

# Display plots side by side
grid.arrange(p1, p2, p3, ncol = 3, top = "Variable Importance for Neural Net\n")
```

]

---
# Detecting interactions

.pull-left.font80[

- Unfortunately, PDPs can often hide interaction effects.

- Our data: $$Y = 10 \sin\left(\pi x_1 x_2\right) + 20 \left(x_3 - 0.5\right) ^ 2 + 10 x_4 + 5 x_5 + \epsilon$$

- Our PDP-based VIP approach does a better job identifying actual influential features than established methods

- But it doesn't highlight the interaction effect

- However, PDPs allow you to do .bold[_pairwise interaction partial dependence_]

]

.pull-right[

<br><br>

```{r, echo=FALSE, fig.height=5, message=FALSE}
library(doParallel)  # any parallel backend supported by the foreach package should work
registerDoParallel(cores = 4)  # use four cores
library(gbm)
library(vip)

# Fit a GBM
set.seed(937)
trn.gbm <- gbm(y ~ ., data = trn, distribution = "gaussian", n.trees = 25000,
               shrinkage = 0.01, interaction.depth = 2, bag.fraction = 1,
               train.fraction = 0.8, cv.folds = 5, verbose = FALSE)
best.iter <- gbm.perf(trn.gbm, method = "cv", plot.it = FALSE)

# Friedman's H-statistic
combns <- t(combn(paste0("x.", 1:10), m = 2))
int.h <- numeric(nrow(combns))
for (i in 1:nrow(combns)) {
  # print(paste("iter", i, "of", nrow(combns)))
  int.h[i] <- interact.gbm(trn.gbm, data = trn, i.var = combns[i, ], 
                           n.trees = best.iter)
}
int.h <- data.frame(x = paste0(combns[, 1L], "*", combns[, 2L]), y = int.h)
int.h <- int.h[order(int.h$y, decreasing = TRUE), ]

# VI-based interaction statistic
int.i <- vint(
  object = trn.gbm,                    # fitted model object
  feature_names = paste0("x.", 1:10),  # features for which to compute pairwise interactions statistics
  n.trees = best.iter,                 # needed if object is of class "gbm"
  parallel = TRUE
)

# Plot Friedman's H-statistics
p1 <- ggplot(int.h[1:10, ], aes(reorder(x, y), y)) +
  geom_col(width = 0.75) +
  labs(x = "", y = "Interaction strength", title = "Friedman's H-statistic") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1)) +
  theme_light() +
  coord_flip()

# Plot PDP-based interaction statistics
p2 <- ggplot(int.i[1:10, ], aes(reorder(Variables, Interaction), Interaction)) +
  geom_col(width = 0.75) +
  labs(x = "", y = "Interaction strength", title = "PDP method") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1)) +
  theme_light() +
  coord_flip()

# Display plots side by side
grid.arrange(p2, p1, ncol = 2)
```

]

---
# A solution

<br>

.pull-left[

.bold[Necessary attributes]

- Simple model-agnostic approach `r fontawesome::fa("check", height = 15)`

- Easy to interpret `r fontawesome::fa("check", height = 15)`

- Scales well `r fontawesome::fa("check", height = 15)`

- Detects interactions `r fontawesome::fa("check", height = 15)`

- Creates a consistent implementation

]

.pull-right[

.bold[Result]

1. PDP-based feature importance measure `r fontawesome::fa("check", height = 15)`

2. Open source tool for feature importance analysis

]

---
class: section-slide 

<br><br><br><br><br><br><br><br><br><br><br>
# .font150[A single interface] <img src="figures/logo-vip.png" align="right">

---
# Consistent interface

.pull-left[

Even within a single language:

- each model requires unique approach to get variable importance data
- output object differs based on model

]

.scrollable90.pull-right[

```{r, echo=FALSE, results='hide'}
# Simulate training data
set.seed(101)  # for reproducibility
trn <- as.data.frame(mlbench::mlbench.friedman1(500))

# Load required packages
library(xgboost)  # for fitting GBMs
library(ranger)   # for fitting random forests
library(rpart)    # for fitting CART-like decision trees
library(nnet)     # feed forward neural network
library(earth)
library(ggplot2)

# Fit a single regression tree
tree <- rpart(y ~ ., data = trn)

# Fit a random forest
set.seed(101)
rf <- ranger(y ~ ., data = trn, importance = "impurity")

# Fit a GBM
set.seed(102)
gbm <- xgboost(
  data = data.matrix(subset(trn, select = -y)),
  label = trn$y, 
  objective = "reg:linear",
  nrounds = 100, 
  max_depth = 5, 
  eta = 0.3,
  verbose = 0  # suppress printing
)

# Fit a LM
lm <- lm(y ~ .^2, data = trn)
backward <- step(lm, direction = "backward", trace = 0)

# Fit a neural network
set.seed(0803)
nn <- nnet(y ~ ., data = trn, size = 7, decay = 0.1, linout = TRUE)

# Fit a MARS model
mars <- earth(y ~ ., data = trn, degree = 2, pmethod = "exhaustive")
```

```{r, collapse=TRUE}
# VI for RF
rf$variable.importance

# VI for GBM
xgb.importance(model = gbm)

## VI for MARS
earth::evimp(mars)

# VI for LM
broom::tidy(lm)[, c("term", "statistic")]

# VI for NN
NeuralNetTools::garson(nn, bar_plot = FALSE)
```


]

---
# Consistent interface

.pull-left[

Data scientists need to iterate, develop, and compare quickly by way of:

]

---
# Consistent interface

.pull-left[

Data scientists need to iterate, develop, and compare quickly by way of:

* .bold[Consistent computation regardless of model type]

]

.pull-right.scrollable90[

```{r, collapse=TRUE, message=FALSE}
library(vip)

# VI for RF
vi(rf, scale = TRUE)

# VI for GBM
vi(gbm, scale = TRUE)

## VI for MARS
vi(mars, scale = TRUE)

# VI for LM
vi(lm, scale = TRUE)

# VI for NN
vi(nn, scale = TRUE)
```

]

---
# Consistent interface

.pull-left[

Data scientists need to iterate, develop, and compare quickly by way of:

* Consistent computation regardless of model type

* .bold[Consistent visualizations to compare]

]

.pull-right.scrollable90[

```{r, fig.height=5}
# Plot VI scores
p1 <- vip(lm, scale = TRUE) + ggtitle("Linear Regression")
p2 <- vip(rf, scale = TRUE) + ggtitle("Random Forest")
p3 <- vip(gbm, scale = TRUE) + ggtitle("XGBoost")
p4 <- vip(nn, scale = TRUE) + ggtitle("Neural Net")

# Display plots side by side
grid.arrange(p1, p2, p3, p4, ncol = 2)
```

]

---
# Consistent interface

.pull-left[

Data scientists need to iterate, develop, and compare quickly by way of:

* Consistent computation regardless of model type

* Consistent visualizations to compare

* .bold[Consistent implementation of different methods]

]

.pull-right.scrollable90[

```{r, fig.height=5}
# Plot VI scores via different methods
p1 <- vip(nn, method = "model", scale = TRUE) + ggtitle("Model-based VIP")
p2 <- vip(nn, method = "firm", scale = TRUE) + ggtitle("PDP-based VIP")
p3 <- vip(nn, method = "permute", scale = TRUE, metric = "rsquared", target = "y", pred_wrapper = predict) + ggtitle("Permutation VIP")
p4 <- vip(nn, method = "shap", scale = TRUE, pred_wrapper = predict) + ggtitle("SHAP VIP")

# Display plots side by side
grid.arrange(p1, p2, p3, p4, ncol = 2)
```

]

---
# Consistent interface

.pull-left[

Data scientists need to iterate, develop, and compare quickly by way of:

* Consistent computation regardless of model type

* Consistent visualizations to compare

* Consistent implementation of different methods

* .bold[Plus some extra bells & whistles]

]

.pull-right.scrollable90.code80[

```{r, out.height="150%"}
# Sparkline table for easy reporting
nn %>%
  vi(method = "firm") %>%
  add_sparklines()
```

]

---
# Consistent interface

.pull-left[

Data scientists need to iterate, develop, and compare quickly by way of:

* Consistent computation regardless of model type

* Consistent visualizations to compare

* Consistent implementation of different methods

* Plus some extra bells & whistles

]

.pull-right.scrollable90.code80[

.bold[Result:]

<img src="https://cranlogs.r-pkg.org/badges/vip" style="border: 0; display:inline; margin: 0 2px; box-shadow: none">

<img src="https://cranlogs.r-pkg.org/badges/grand-total/vip" style="border: 0; display:inline; margin: 0 2px; box-shadow: none">

]

---
class: section-slide 

<br><br><br><br><br><br><br><br><br><br><br>
# .font150[Improvements to be had]

---
# Where to go from here

<br>
* Advancing IML methods
   - statistical
   - visual
   - .bold[_scale_]
   - prediction intervals

* Model quality vs. IML trust

* IML for non-supervised modeling

* Language agnostic tooling

* Decision support tooling

---
class: section-slide 

<br><br><br><br><br><br><br><br><br><br><br>
# .font150[Thank You!]
